{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL-D--JUmTQU",
        "outputId": "b33214c3-b804-4f21-c697-f199873755c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 2 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.12   2         auto mode\n",
            "  1            /usr/bin/python3.10   1         manual mode\n",
            "  2            /usr/bin/python3.12   2         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 1\n",
            "update-alternatives: using /usr/bin/python3.10 to provide /usr/bin/python3 (python3) in manual mode\n",
            "/usr/bin/python3: No module named pip\n",
            "/usr/bin/python3: No module named pip\n",
            "/usr/bin/python3\n",
            "Python 3.10.12\n",
            "--2025-12-03 00:41:18--  https://bootstrap.pypa.io/get-pip.py\n",
            "Resolving bootstrap.pypa.io (bootstrap.pypa.io)... 151.101.0.175, 151.101.64.175, 151.101.128.175, ...\n",
            "Connecting to bootstrap.pypa.io (bootstrap.pypa.io)|151.101.0.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2182415 (2.1M) [text/x-python]\n",
            "Saving to: ‘get-pip.py’\n",
            "\n",
            "get-pip.py          100%[===================>]   2.08M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-12-03 00:41:18 (88.2 MB/s) - ‘get-pip.py’ saved [2182415/2182415]\n",
            "\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pip]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pip-25.3 setuptools-80.9.0 wheel-0.45.1\n",
            "pip 25.3 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting gymnasium==0.29.1 (from gymnasium[atari]==0.29.1)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting numpy>=1.21.0 (from gymnasium==0.29.1->gymnasium[atari]==0.29.1)\n",
            "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting cloudpickle>=1.2.0 (from gymnasium==0.29.1->gymnasium[atari]==0.29.1)\n",
            "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-extensions>=4.3.0 (from gymnasium==0.29.1->gymnasium[atari]==0.29.1)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==0.29.1->gymnasium[atari]==0.29.1)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1)\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1)\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting importlib-resources (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1)\n",
            "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: farama-notifications, typing-extensions, numpy, importlib-resources, cloudpickle, gymnasium, ale-py, shimmy\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [shimmy]\n",
            "\u001b[1A\u001b[2KSuccessfully installed ale-py-0.8.1 cloudpickle-3.1.2 farama-notifications-0.0.4 gymnasium-0.29.1 importlib-resources-6.5.2 numpy-2.2.6 shimmy-0.2.1 typing-extensions-4.15.0\n",
            "Requirement already satisfied: ale-py==0.8.1 in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py==0.8.1) (2.2.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py==0.8.1) (6.5.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py==0.8.1) (4.15.0)\n",
            "Collecting autorom\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting click (from autorom)\n",
            "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting requests (from autorom)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->autorom)\n",
            "  Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->autorom)\n",
            "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->autorom)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->autorom)\n",
            "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "Installing collected packages: urllib3, idna, click, charset_normalizer, certifi, requests, autorom\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [autorom]\n",
            "\u001b[1A\u001b[2KSuccessfully installed autorom-0.6.1 certifi-2025.11.12 charset_normalizer-3.4.4 click-8.3.1 idna-3.11 requests-2.32.5 urllib3-2.5.0\n",
            "/usr/bin/python3: No module named AutoROM.__main__; 'AutoROM' is a package and cannot be directly executed\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: AutoROM in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py) (6.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from AutoROM) (8.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from AutoROM) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->AutoROM) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->AutoROM) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->AutoROM) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->AutoROM) (2025.11.12)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446709 sha256=aadccdc91f03baee7b3fc9245da33d2e86781a610f919b3ee62768a436d1b4c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1\n",
            "/usr/bin/python3: No module named AutoROM.accept_rom_license\n",
            "Atari envs registered!\n",
            "===== classic_control =====\n",
            "Acrobot-v1                  CartPole-v0                 CartPole-v1\n",
            "MountainCar-v0              MountainCarContinuous-v0    Pendulum-v1\n",
            "===== phys2d =====\n",
            "phys2d/CartPole-v0          phys2d/CartPole-v1          phys2d/Pendulum-v0\n",
            "===== box2d =====\n",
            "BipedalWalker-v3            BipedalWalkerHardcore-v3    CarRacing-v3\n",
            "LunarLander-v3              LunarLanderContinuous-v3\n",
            "===== toy_text =====\n",
            "Blackjack-v1                CliffWalking-v1             CliffWalkingSlippery-v1\n",
            "FrozenLake-v1               FrozenLake8x8-v1            Taxi-v3\n",
            "===== tabular =====\n",
            "tabular/Blackjack-v0        tabular/CliffWalking-v0\n",
            "===== None =====\n",
            "Ant-v2                      Ant-v3                      GymV21Environment-v0\n",
            "GymV26Environment-v0        HalfCheetah-v2              HalfCheetah-v3\n",
            "Hopper-v2                   Hopper-v3                   Humanoid-v2\n",
            "Humanoid-v3                 HumanoidStandup-v2          InvertedDoublePendulum-v2\n",
            "InvertedPendulum-v2         Pusher-v2                   Reacher-v2\n",
            "Swimmer-v2                  Swimmer-v3                  Walker2d-v2\n",
            "Walker2d-v3\n",
            "===== mujoco =====\n",
            "Ant-v4                      Ant-v5                      HalfCheetah-v4\n",
            "HalfCheetah-v5              Hopper-v4                   Hopper-v5\n",
            "Humanoid-v4                 Humanoid-v5                 HumanoidStandup-v4\n",
            "HumanoidStandup-v5          InvertedDoublePendulum-v4   InvertedDoublePendulum-v5\n",
            "InvertedPendulum-v4         InvertedPendulum-v5         Pusher-v4\n",
            "Pusher-v5                   Reacher-v4                  Reacher-v5\n",
            "Swimmer-v4                  Swimmer-v5                  Walker2d-v4\n",
            "Walker2d-v5\n",
            "===== env =====\n",
            "Adventure-v0                Adventure-v4                AdventureNoFrameskip-v0\n",
            "AdventureNoFrameskip-v4     AirRaid-v0                  AirRaid-v4\n",
            "AirRaidNoFrameskip-v0       AirRaidNoFrameskip-v4       Alien-v0\n",
            "Alien-v4                    AlienNoFrameskip-v0         AlienNoFrameskip-v4\n",
            "Amidar-v0                   Amidar-v4                   AmidarNoFrameskip-v0\n",
            "AmidarNoFrameskip-v4        Assault-v0                  Assault-v4\n",
            "AssaultNoFrameskip-v0       AssaultNoFrameskip-v4       Asterix-v0\n",
            "Asterix-v4                  AsterixNoFrameskip-v0       AsterixNoFrameskip-v4\n",
            "Asteroids-v0                Asteroids-v4                AsteroidsNoFrameskip-v0\n",
            "AsteroidsNoFrameskip-v4     Atlantis-v0                 Atlantis-v4\n",
            "AtlantisNoFrameskip-v0      AtlantisNoFrameskip-v4      BankHeist-v0\n",
            "BankHeist-v4                BankHeistNoFrameskip-v0     BankHeistNoFrameskip-v4\n",
            "BattleZone-v0               BattleZone-v4               BattleZoneNoFrameskip-v0\n",
            "BattleZoneNoFrameskip-v4    BeamRider-v0                BeamRider-v4\n",
            "BeamRiderNoFrameskip-v0     BeamRiderNoFrameskip-v4     Berzerk-v0\n",
            "Berzerk-v4                  BerzerkNoFrameskip-v0       BerzerkNoFrameskip-v4\n",
            "Bowling-v0                  Bowling-v4                  BowlingNoFrameskip-v0\n",
            "BowlingNoFrameskip-v4       Boxing-v0                   Boxing-v4\n",
            "BoxingNoFrameskip-v0        BoxingNoFrameskip-v4        Breakout-v0\n",
            "Breakout-v4                 BreakoutNoFrameskip-v0      BreakoutNoFrameskip-v4\n",
            "Carnival-v0                 Carnival-v4                 CarnivalNoFrameskip-v0\n",
            "CarnivalNoFrameskip-v4      Centipede-v0                Centipede-v4\n",
            "CentipedeNoFrameskip-v0     CentipedeNoFrameskip-v4     ChopperCommand-v0\n",
            "ChopperCommand-v4           ChopperCommandNoFrameskip-v0 ChopperCommandNoFrameskip-v4\n",
            "CrazyClimber-v0             CrazyClimber-v4             CrazyClimberNoFrameskip-v0\n",
            "CrazyClimberNoFrameskip-v4  Defender-v0                 Defender-v4\n",
            "DefenderNoFrameskip-v0      DefenderNoFrameskip-v4      DemonAttack-v0\n",
            "DemonAttack-v4              DemonAttackNoFrameskip-v0   DemonAttackNoFrameskip-v4\n",
            "DoubleDunk-v0               DoubleDunk-v4               DoubleDunkNoFrameskip-v0\n",
            "DoubleDunkNoFrameskip-v4    ElevatorAction-v0           ElevatorAction-v4\n",
            "ElevatorActionNoFrameskip-v0 ElevatorActionNoFrameskip-v4 Enduro-v0\n",
            "Enduro-v4                   EnduroNoFrameskip-v0        EnduroNoFrameskip-v4\n",
            "FishingDerby-v0             FishingDerby-v4             FishingDerbyNoFrameskip-v0\n",
            "FishingDerbyNoFrameskip-v4  Freeway-v0                  Freeway-v4\n",
            "FreewayNoFrameskip-v0       FreewayNoFrameskip-v4       Frostbite-v0\n",
            "Frostbite-v4                FrostbiteNoFrameskip-v0     FrostbiteNoFrameskip-v4\n",
            "Gopher-v0                   Gopher-v4                   GopherNoFrameskip-v0\n",
            "GopherNoFrameskip-v4        Gravitar-v0                 Gravitar-v4\n",
            "GravitarNoFrameskip-v0      GravitarNoFrameskip-v4      Hero-v0\n",
            "Hero-v4                     HeroNoFrameskip-v0          HeroNoFrameskip-v4\n",
            "IceHockey-v0                IceHockey-v4                IceHockeyNoFrameskip-v0\n",
            "IceHockeyNoFrameskip-v4     Jamesbond-v0                Jamesbond-v4\n",
            "JamesbondNoFrameskip-v0     JamesbondNoFrameskip-v4     JourneyEscape-v0\n",
            "JourneyEscape-v4            JourneyEscapeNoFrameskip-v0 JourneyEscapeNoFrameskip-v4\n",
            "Kangaroo-v0                 Kangaroo-v4                 KangarooNoFrameskip-v0\n",
            "KangarooNoFrameskip-v4      Krull-v0                    Krull-v4\n",
            "KrullNoFrameskip-v0         KrullNoFrameskip-v4         KungFuMaster-v0\n",
            "KungFuMaster-v4             KungFuMasterNoFrameskip-v0  KungFuMasterNoFrameskip-v4\n",
            "MontezumaRevenge-v0         MontezumaRevenge-v4         MontezumaRevengeNoFrameskip-v0\n",
            "MontezumaRevengeNoFrameskip-v4 MsPacman-v0                 MsPacman-v4\n",
            "MsPacmanNoFrameskip-v0      MsPacmanNoFrameskip-v4      NameThisGame-v0\n",
            "NameThisGame-v4             NameThisGameNoFrameskip-v0  NameThisGameNoFrameskip-v4\n",
            "Phoenix-v0                  Phoenix-v4                  PhoenixNoFrameskip-v0\n",
            "PhoenixNoFrameskip-v4       Pitfall-v0                  Pitfall-v4\n",
            "PitfallNoFrameskip-v0       PitfallNoFrameskip-v4       Pong-v0\n",
            "Pong-v4                     PongNoFrameskip-v0          PongNoFrameskip-v4\n",
            "Pooyan-v0                   Pooyan-v4                   PooyanNoFrameskip-v0\n",
            "PooyanNoFrameskip-v4        PrivateEye-v0               PrivateEye-v4\n",
            "PrivateEyeNoFrameskip-v0    PrivateEyeNoFrameskip-v4    Qbert-v0\n",
            "Qbert-v4                    QbertNoFrameskip-v0         QbertNoFrameskip-v4\n",
            "Riverraid-v0                Riverraid-v4                RiverraidNoFrameskip-v0\n",
            "RiverraidNoFrameskip-v4     RoadRunner-v0               RoadRunner-v4\n",
            "RoadRunnerNoFrameskip-v0    RoadRunnerNoFrameskip-v4    Robotank-v0\n",
            "Robotank-v4                 RobotankNoFrameskip-v0      RobotankNoFrameskip-v4\n",
            "Seaquest-v0                 Seaquest-v4                 SeaquestNoFrameskip-v0\n",
            "SeaquestNoFrameskip-v4      Skiing-v0                   Skiing-v4\n",
            "SkiingNoFrameskip-v0        SkiingNoFrameskip-v4        Solaris-v0\n",
            "Solaris-v4                  SolarisNoFrameskip-v0       SolarisNoFrameskip-v4\n",
            "SpaceInvaders-v0            SpaceInvaders-v4            SpaceInvadersNoFrameskip-v0\n",
            "SpaceInvadersNoFrameskip-v4 StarGunner-v0               StarGunner-v4\n",
            "StarGunnerNoFrameskip-v0    StarGunnerNoFrameskip-v4    Tennis-v0\n",
            "Tennis-v4                   TennisNoFrameskip-v0        TennisNoFrameskip-v4\n",
            "TimePilot-v0                TimePilot-v4                TimePilotNoFrameskip-v0\n",
            "TimePilotNoFrameskip-v4     Tutankham-v0                Tutankham-v4\n",
            "TutankhamNoFrameskip-v0     TutankhamNoFrameskip-v4     UpNDown-v0\n",
            "UpNDown-v4                  UpNDownNoFrameskip-v0       UpNDownNoFrameskip-v4\n",
            "Venture-v0                  Venture-v4                  VentureNoFrameskip-v0\n",
            "VentureNoFrameskip-v4       VideoPinball-v0             VideoPinball-v4\n",
            "VideoPinballNoFrameskip-v0  VideoPinballNoFrameskip-v4  WizardOfWor-v0\n",
            "WizardOfWor-v4              WizardOfWorNoFrameskip-v0   WizardOfWorNoFrameskip-v4\n",
            "YarsRevenge-v0              YarsRevenge-v4              YarsRevengeNoFrameskip-v0\n",
            "YarsRevengeNoFrameskip-v4   Zaxxon-v0                   Zaxxon-v4\n",
            "ZaxxonNoFrameskip-v0        ZaxxonNoFrameskip-v4\n",
            "===== ALE =====\n",
            "ALE/Adventure-v5            ALE/AirRaid-v5              ALE/Alien-v5\n",
            "ALE/Amidar-v5               ALE/Assault-v5              ALE/Asterix-v5\n",
            "ALE/Asteroids-v5            ALE/Atlantis-v5             ALE/Atlantis2-v5\n",
            "ALE/Backgammon-v5           ALE/BankHeist-v5            ALE/BasicMath-v5\n",
            "ALE/BattleZone-v5           ALE/BeamRider-v5            ALE/Berzerk-v5\n",
            "ALE/Blackjack-v5            ALE/Bowling-v5              ALE/Boxing-v5\n",
            "ALE/Breakout-v5             ALE/Carnival-v5             ALE/Casino-v5\n",
            "ALE/Centipede-v5            ALE/ChopperCommand-v5       ALE/CrazyClimber-v5\n",
            "ALE/Crossbow-v5             ALE/Darkchambers-v5         ALE/Defender-v5\n",
            "ALE/DemonAttack-v5          ALE/DonkeyKong-v5           ALE/DoubleDunk-v5\n",
            "ALE/Earthworld-v5           ALE/ElevatorAction-v5       ALE/Enduro-v5\n",
            "ALE/Entombed-v5             ALE/Et-v5                   ALE/FishingDerby-v5\n",
            "ALE/FlagCapture-v5          ALE/Freeway-v5              ALE/Frogger-v5\n",
            "ALE/Frostbite-v5            ALE/Galaxian-v5             ALE/Gopher-v5\n",
            "ALE/Gravitar-v5             ALE/Hangman-v5              ALE/HauntedHouse-v5\n",
            "ALE/Hero-v5                 ALE/HumanCannonball-v5      ALE/IceHockey-v5\n",
            "ALE/Jamesbond-v5            ALE/JourneyEscape-v5        ALE/Kaboom-v5\n",
            "ALE/Kangaroo-v5             ALE/KeystoneKapers-v5       ALE/KingKong-v5\n",
            "ALE/Klax-v5                 ALE/Koolaid-v5              ALE/Krull-v5\n",
            "ALE/KungFuMaster-v5         ALE/LaserGates-v5           ALE/LostLuggage-v5\n",
            "ALE/MarioBros-v5            ALE/MiniatureGolf-v5        ALE/MontezumaRevenge-v5\n",
            "ALE/MrDo-v5                 ALE/MsPacman-v5             ALE/NameThisGame-v5\n",
            "ALE/Othello-v5              ALE/Pacman-v5               ALE/Phoenix-v5\n",
            "ALE/Pitfall-v5              ALE/Pitfall2-v5             ALE/Pong-v5\n",
            "ALE/Pooyan-v5               ALE/PrivateEye-v5           ALE/Qbert-v5\n",
            "ALE/Riverraid-v5            ALE/RoadRunner-v5           ALE/Robotank-v5\n",
            "ALE/Seaquest-v5             ALE/SirLancelot-v5          ALE/Skiing-v5\n",
            "ALE/Solaris-v5              ALE/SpaceInvaders-v5        ALE/SpaceWar-v5\n",
            "ALE/StarGunner-v5           ALE/Superman-v5             ALE/Surround-v5\n",
            "ALE/Tennis-v5               ALE/Tetris-v5               ALE/TicTacToe3D-v5\n",
            "ALE/TimePilot-v5            ALE/Trondead-v5             ALE/Turmoil-v5\n",
            "ALE/Tutankham-v5            ALE/UpNDown-v5              ALE/Venture-v5\n",
            "ALE/VideoCheckers-v5        ALE/VideoChess-v5           ALE/VideoCube-v5\n",
            "ALE/VideoPinball-v5         ALE/WizardOfWor-v5          ALE/WordZapper-v5\n",
            "ALE/YarsRevenge-v5          ALE/Zaxxon-v5\n"
          ]
        }
      ],
      "source": [
        "!sudo update-alternatives --config python3\n",
        "!python3 -m pip install --upgrade pip\n",
        "!python3 -m pip install \"gymnasium[atari]\" ale-py autorom\n",
        "!which python3\n",
        "!python3 --version\n",
        "!wget https://bootstrap.pypa.io/get-pip.py\n",
        "!python3 get-pip.py\n",
        "!python3 -m pip --version\n",
        "!python3 -m pip install \"gymnasium[atari]==0.29.1\"\n",
        "!python3 -m pip install \"ale-py==0.8.1\"\n",
        "!python3 -m pip install autorom\n",
        "!python3 -m AutoROM --accept-license\n",
        "!python3 -m pip install \"gymnasium[atari]\" ale-py AutoROM AutoROM.accept-rom-license\n",
        "\n",
        "!python3 -m AutoROM.accept_rom_license --accept-license\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "print(\"Atari envs registered!\")\n",
        "\n",
        "gym.pprint_registry()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-_UPP-mnMdV",
        "outputId": "28db8951-1c84-409e-bb6a-4f0a52532fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium==0.29.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]==0.29.1) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (2.2.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->gymnasium[atari]==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1) (0.2.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]==0.29.1) (6.5.2)\n",
            "Requirement already satisfied: ale-py==0.8.1 in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py==0.8.1) (2.2.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py==0.8.1) (6.5.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py==0.8.1) (4.15.0)\n",
            "Requirement already satisfied: autorom in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom) (8.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"gymnasium[atari]==0.29.1\"\n",
        "!pip install \"ale-py==0.8.1\"\n",
        "!pip install autorom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK2HwXBevf-0",
        "outputId": "a74603f7-5460-47dc-e422-cd2be21c627d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Observation space: Box(0, 255, (4, 84, 84), uint8)\n",
            "Action space: Discrete(6)\n",
            "Total updates = 1220\n",
            "[Update 20/1220] AvgReturn (last 50): 188.30\n",
            "[Update 40/1220] AvgReturn (last 50): 195.60\n",
            "[Update 60/1220] AvgReturn (last 50): 252.30\n",
            "[Update 80/1220] AvgReturn (last 50): 271.40\n",
            "[Update 100/1220] AvgReturn (last 50): 296.80\n",
            "[Update 120/1220] AvgReturn (last 50): 336.90\n",
            "[Update 140/1220] AvgReturn (last 50): 356.80\n",
            "[Update 160/1220] AvgReturn (last 50): 400.60\n",
            "[Update 180/1220] AvgReturn (last 50): 365.60\n",
            "[Update 200/1220] AvgReturn (last 50): 364.40\n",
            "[Update 220/1220] AvgReturn (last 50): 376.10\n",
            "[Update 240/1220] AvgReturn (last 50): 370.70\n",
            "[Update 260/1220] AvgReturn (last 50): 352.00\n",
            "[Update 280/1220] AvgReturn (last 50): 410.10\n",
            "[Update 300/1220] AvgReturn (last 50): 447.10\n",
            "[Update 320/1220] AvgReturn (last 50): 451.70\n",
            "[Update 340/1220] AvgReturn (last 50): 399.30\n",
            "[Update 360/1220] AvgReturn (last 50): 425.90\n",
            "[Update 380/1220] AvgReturn (last 50): 440.00\n",
            "[Update 400/1220] AvgReturn (last 50): 444.90\n",
            "[Update 420/1220] AvgReturn (last 50): 475.30\n",
            "[Update 440/1220] AvgReturn (last 50): 484.30\n",
            "[Update 460/1220] AvgReturn (last 50): 453.60\n",
            "[Update 480/1220] AvgReturn (last 50): 455.90\n",
            "[Update 500/1220] AvgReturn (last 50): 468.80\n",
            "[Update 520/1220] AvgReturn (last 50): 487.20\n",
            "[Update 540/1220] AvgReturn (last 50): 512.20\n",
            "[Update 560/1220] AvgReturn (last 50): 467.50\n",
            "[Update 580/1220] AvgReturn (last 50): 485.40\n",
            "[Update 600/1220] AvgReturn (last 50): 508.20\n",
            "[Update 620/1220] AvgReturn (last 50): 475.80\n",
            "[Update 640/1220] AvgReturn (last 50): 511.40\n",
            "[Update 660/1220] AvgReturn (last 50): 514.00\n",
            "[Update 680/1220] AvgReturn (last 50): 511.00\n",
            "[Update 700/1220] AvgReturn (last 50): 537.20\n",
            "[Update 720/1220] AvgReturn (last 50): 544.00\n",
            "[Update 740/1220] AvgReturn (last 50): 536.70\n",
            "[Update 760/1220] AvgReturn (last 50): 580.30\n",
            "[Update 780/1220] AvgReturn (last 50): 558.20\n",
            "[Update 800/1220] AvgReturn (last 50): 545.40\n",
            "[Update 820/1220] AvgReturn (last 50): 547.70\n",
            "[Update 840/1220] AvgReturn (last 50): 522.10\n",
            "[Update 860/1220] AvgReturn (last 50): 537.20\n",
            "[Update 880/1220] AvgReturn (last 50): 582.80\n",
            "[Update 900/1220] AvgReturn (last 50): 563.70\n",
            "[Update 920/1220] AvgReturn (last 50): 572.00\n",
            "[Update 940/1220] AvgReturn (last 50): 558.70\n",
            "[Update 960/1220] AvgReturn (last 50): 537.40\n",
            "[Update 980/1220] AvgReturn (last 50): 582.60\n",
            "[Update 1000/1220] AvgReturn (last 50): 589.90\n",
            "[Update 1020/1220] AvgReturn (last 50): 628.10\n",
            "[Update 1040/1220] AvgReturn (last 50): 586.00\n",
            "[Update 1060/1220] AvgReturn (last 50): 606.20\n",
            "[Update 1080/1220] AvgReturn (last 50): 558.30\n",
            "[Update 1100/1220] AvgReturn (last 50): 567.60\n",
            "[Update 1120/1220] AvgReturn (last 50): 607.30\n",
            "[Update 1140/1220] AvgReturn (last 50): 578.00\n",
            "[Update 1160/1220] AvgReturn (last 50): 576.50\n",
            "[Update 1180/1220] AvgReturn (last 50): 583.40\n",
            "[Update 1200/1220] AvgReturn (last 50): 605.10\n",
            "[Update 1220/1220] AvgReturn (last 50): 594.80\n",
            "\n",
            "Training completed!\n",
            "Saved: spaceinvaders_ppo.pth + spaceinvaders_ppo_returns.npy\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "#  Utils\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "#  PPO CNN Policy for Atari\n",
        "\n",
        "class PPOAtariPolicy(nn.Module):\n",
        "    def __init__(self, action_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input: (B, 4, 84, 84)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(3136, 512), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.pi = nn.Linear(512, action_dim)\n",
        "        self.v  = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        logits = self.pi(x)\n",
        "        value  = self.v(x).squeeze(-1)\n",
        "        return logits, value\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        logits, value = self.forward(x)\n",
        "        dist = Categorical(logits=logits)\n",
        "\n",
        "        if action is None:\n",
        "            action = dist.sample()\n",
        "\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy  = dist.entropy()\n",
        "        return action, log_prob, entropy, value\n",
        "\n",
        "#  Environment helpers (SPACE INVADERS)\n",
        "\n",
        "def make_single_spaceinvader_env(seed: int = 0):\n",
        "    \"\"\"\n",
        "    Creates one SpaceInvaders environment with:\n",
        "    - grayscale\n",
        "    - frame-skip=4\n",
        "    - resize to 84x84\n",
        "    - stacked 4 frames (84,84,4)\n",
        "    \"\"\"\n",
        "\n",
        "    def thunk():\n",
        "        env = gym.make(\n",
        "            \"ALE/SpaceInvaders-v5\",\n",
        "            frameskip=1,\n",
        "            repeat_action_probability=0.0,\n",
        "            render_mode=None\n",
        "        )\n",
        "\n",
        "        env = AtariPreprocessing(\n",
        "            env,\n",
        "            frame_skip=4,\n",
        "            grayscale_obs=True,\n",
        "            screen_size=84,\n",
        "            scale_obs=False,\n",
        "        )\n",
        "\n",
        "        # produce obs shape (84,84,4)\n",
        "        env = FrameStackObservation(env, stack_size=4)\n",
        "\n",
        "        env.reset(seed=seed)\n",
        "        return env\n",
        "\n",
        "    return thunk\n",
        "\n",
        "\n",
        "def preprocess_obs(obs: np.ndarray):\n",
        "    \"\"\"\n",
        "    For SpaceInvaders:\n",
        "    env observation space is (4, 84, 84), i.e. channels-first already.\n",
        "    Vectorized env gives (num_envs, 4, 84, 84).\n",
        "\n",
        "    We just normalize to [0,1] and keep the shape as (N,4,84,84).\n",
        "    \"\"\"\n",
        "    obs_t = torch.from_numpy(obs).float().to(DEVICE) / 255.0\n",
        "    return obs_t\n",
        "\n",
        "#  PPO TRAINING LOOP\n",
        "\n",
        "def train_ppo_spaceinvaders(\n",
        "    total_timesteps: int = 8_000_000,\n",
        "    n_envs: int = 16,\n",
        "    num_steps: int = 256,\n",
        "    gamma: float = 0.99,\n",
        "    gae_lambda: float = 0.95,\n",
        "    clip_coef: float = 0.1,\n",
        "    lr: float = 2.5e-4,\n",
        "    update_epochs: int = 4,\n",
        "    minibatch_size: int = 256,\n",
        "    ent_coef: float = 0.01,\n",
        "    vf_coef: float = 0.5,\n",
        "    max_grad_norm: float = 0.5,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Vectorized envs\n",
        "    env_fns = [make_single_spaceinvader_env(seed + i) for i in range(n_envs)]\n",
        "    envs = gym.vector.SyncVectorEnv(env_fns)\n",
        "\n",
        "    print(\"Observation space:\", envs.single_observation_space)\n",
        "    print(\"Action space:\", envs.single_action_space)\n",
        "\n",
        "    # Action space size\n",
        "    action_dim = envs.single_action_space.n\n",
        "\n",
        "    # PPO Model\n",
        "    model = PPOAtariPolicy(action_dim).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-5)\n",
        "\n",
        "    # Rollout buffers\n",
        "    obs_buf      = torch.zeros(num_steps, n_envs, 4, 84, 84, device=DEVICE)\n",
        "    actions_buf  = torch.zeros(num_steps, n_envs, dtype=torch.long, device=DEVICE)\n",
        "    logprobs_buf = torch.zeros(num_steps, n_envs, device=DEVICE)\n",
        "    rewards_buf  = torch.zeros(num_steps, n_envs, device=DEVICE)\n",
        "    dones_buf    = torch.zeros(num_steps, n_envs, device=DEVICE)\n",
        "    values_buf   = torch.zeros(num_steps, n_envs, device=DEVICE)\n",
        "\n",
        "    # Initial obs\n",
        "    obs, _ = envs.reset()\n",
        "    obs_t = preprocess_obs(obs)\n",
        "\n",
        "    episode_returns = np.zeros(n_envs, np.float32)\n",
        "    episode_lengths = np.zeros(n_envs, np.int32)\n",
        "\n",
        "    finished_returns = []\n",
        "\n",
        "    num_updates = total_timesteps // (n_envs * num_steps)\n",
        "    print(f\"Total updates = {num_updates}\")\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "\n",
        "        #  COLLECT ROLLOUT\n",
        "        for step in range(num_steps):\n",
        "            with torch.no_grad():\n",
        "                action, logp, entropy, value = model.get_action_and_value(obs_t)\n",
        "\n",
        "            actions = action.cpu().numpy()\n",
        "\n",
        "            next_obs, reward, terminated, truncated, infos = envs.step(actions)\n",
        "            done = np.logical_or(terminated, truncated)\n",
        "\n",
        "            reward_clip = np.clip(reward, -1.0, 1.0)\n",
        "\n",
        "            # Store rollout data\n",
        "            obs_buf[step]      = obs_t\n",
        "            actions_buf[step]  = action\n",
        "            logprobs_buf[step] = logp\n",
        "            rewards_buf[step]  = torch.tensor(reward_clip, device=DEVICE).float()\n",
        "            dones_buf[step]    = torch.tensor(done, device=DEVICE).float()\n",
        "            values_buf[step]   = value\n",
        "\n",
        "            # Episode tracking\n",
        "            episode_returns += reward\n",
        "            episode_lengths += 1\n",
        "\n",
        "            for i in range(n_envs):\n",
        "                if done[i]:\n",
        "                    finished_returns.append(episode_returns[i])\n",
        "                    episode_returns[i] = 0.0\n",
        "                    episode_lengths[i] = 0\n",
        "\n",
        "            obs = next_obs\n",
        "            obs_t = preprocess_obs(obs)\n",
        "\n",
        "        #  COMPUTE ADV + RETURNS (GAE)\n",
        "        with torch.no_grad():\n",
        "            _, _, _, next_values = model.get_action_and_value(obs_t)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards_buf)\n",
        "        last_gae = torch.zeros(n_envs, device=DEVICE)\n",
        "\n",
        "        for step in reversed(range(num_steps)):\n",
        "            if step == num_steps - 1:\n",
        "                next_non_terminal = 1.0 - dones_buf[step]\n",
        "                next_vals = next_values\n",
        "            else:\n",
        "                next_non_terminal = 1.0 - dones_buf[step + 1]\n",
        "                next_vals = values_buf[step + 1]\n",
        "\n",
        "            delta = rewards_buf[step] + gamma * next_vals * next_non_terminal - values_buf[step]\n",
        "            last_gae = delta + gamma * gae_lambda * next_non_terminal * last_gae\n",
        "            advantages[step] = last_gae\n",
        "\n",
        "        returns = advantages + values_buf\n",
        "\n",
        "        # Flatten batch\n",
        "        batch_obs = obs_buf.reshape(-1, 4, 84, 84)\n",
        "        batch_actions = actions_buf.reshape(-1)\n",
        "        batch_logprobs = logprobs_buf.reshape(-1)\n",
        "        batch_advantages = advantages.reshape(-1)\n",
        "        batch_returns = returns.reshape(-1)\n",
        "        batch_values = values_buf.reshape(-1)\n",
        "\n",
        "        # Normalize adv\n",
        "        batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-8)\n",
        "        #  PPO UPDATE\n",
        "        batch_size = n_envs * num_steps\n",
        "        indices = np.arange(batch_size)\n",
        "\n",
        "        for epoch in range(update_epochs):\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            for start in range(0, batch_size, minibatch_size):\n",
        "                end = start + minibatch_size\n",
        "                mb_idx = indices[start:end]\n",
        "\n",
        "                mb_obs = batch_obs[mb_idx]\n",
        "                mb_actions = batch_actions[mb_idx]\n",
        "                mb_logprobs_old = batch_logprobs[mb_idx]\n",
        "                mb_adv = batch_advantages[mb_idx]\n",
        "                mb_returns = batch_returns[mb_idx]\n",
        "\n",
        "                _, new_logprob, entropy, new_values = model.get_action_and_value(\n",
        "                    mb_obs, mb_actions\n",
        "                )\n",
        "\n",
        "                ratio = (new_logprob - mb_logprobs_old).exp()\n",
        "\n",
        "                pg1 = -mb_adv * ratio\n",
        "                pg2 = -mb_adv * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
        "                policy_loss = torch.max(pg1, pg2).mean()\n",
        "\n",
        "                value_loss = 0.5 * (mb_returns - new_values).pow(2).mean()\n",
        "\n",
        "                entropy_loss = entropy.mean()\n",
        "\n",
        "                loss = (\n",
        "                    policy_loss +\n",
        "                    vf_coef * value_loss -\n",
        "                    ent_coef * entropy_loss\n",
        "                )\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "        if update % 20 == 0:\n",
        "            avg = np.mean(finished_returns[-50:]) if len(finished_returns) > 0 else 0.0\n",
        "            print(f\"[Update {update}/{num_updates}] AvgReturn (last 50): {avg:.2f}\")\n",
        "\n",
        "    envs.close()\n",
        "\n",
        "    np.save(\"spaceinvaders_ppo_returns.npy\", finished_returns)\n",
        "    torch.save(model.state_dict(), \"spaceinvaders_ppo.pth\")\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "    print(\"Saved: spaceinvaders_ppo.pth + spaceinvaders_ppo_returns.npy\")\n",
        "    return model, finished_returns\n",
        "\n",
        "#  GREEDY EVALUATION\n",
        "\n",
        "def evaluate_ppo_spaceinvaders(model_path=\"spaceinvaders_ppo.pth\", episodes=5, seed=123):\n",
        "    env = make_single_spaceinvader_env(seed)()\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    model = PPOAtariPolicy(action_dim).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.eval()\n",
        "\n",
        "    returns = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env.reset(seed=seed + ep)\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "\n",
        "        while not done:\n",
        "            obs_t = preprocess_obs(obs[None, ...])\n",
        "            with torch.no_grad():\n",
        "                logits, value = model(obs_t)\n",
        "                action = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "\n",
        "        returns.append(total_reward)\n",
        "        print(f\"[EVAL] Episode {ep}: Reward = {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    print(\"Mean reward:\", np.mean(returns))\n",
        "    return returns\n",
        "\n",
        "#  MAIN\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, returns = train_ppo_spaceinvaders(\n",
        "        total_timesteps=5_000_000,\n",
        "        n_envs=16,\n",
        "        num_steps=256,\n",
        "    )\n",
        "\n",
        "    # After training:\n",
        "    # evaluate_ppo_spaceinvaders(\"spaceinvaders_ppo.pth\", episodes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaAQRFmoJmpd",
        "outputId": "192307fd-06f2-47b2-e4ab-2e08d867f40d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[EVAL] Episode 0: Reward = 575.0\n",
            "[EVAL] Episode 1: Reward = 360.0\n",
            "[EVAL] Episode 2: Reward = 465.0\n",
            "[EVAL] Episode 3: Reward = 360.0\n",
            "[EVAL] Episode 4: Reward = 360.0\n",
            "[EVAL] Episode 5: Reward = 575.0\n",
            "[EVAL] Episode 6: Reward = 465.0\n",
            "[EVAL] Episode 7: Reward = 465.0\n",
            "[EVAL] Episode 8: Reward = 285.0\n",
            "[EVAL] Episode 9: Reward = 285.0\n",
            "Mean reward: 419.5\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    #model, returns = train_ppo_spaceinvaders(\n",
        "     #   total_timesteps=5_000_000,\n",
        "      #  n_envs=16,\n",
        "       # num_steps=256,\n",
        "    #)\n",
        "\n",
        "    # After training:\n",
        "     evaluate_ppo_spaceinvaders(\"spaceinvaders_ppo.pth\", episodes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pVZaxozJz3T",
        "outputId": "89cb7a6e-6788-46ac-dde1-15772f092f8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recorded greedy episode: reward=575.0, steps=620\n",
            "Video saved under folder: videos_spaceinvaders_ppo\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation, RecordVideo\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def record_greedy_spaceinvaders_video(\n",
        "    model_path=\"spaceinvaders_ppo.pth\",\n",
        "    video_folder=\"videos_spaceinvaders_ppo\",\n",
        "    seed: int = 123,\n",
        "):\n",
        "    base_env = gym.make(\n",
        "        \"ALE/SpaceInvaders-v5\",\n",
        "        frameskip=1,\n",
        "        repeat_action_probability=0.0,\n",
        "        render_mode=\"rgb_array\",   \n",
        "    )\n",
        "\n",
        "    base_env = AtariPreprocessing(\n",
        "        base_env,\n",
        "        frame_skip=4,\n",
        "        grayscale_obs=True,  \n",
        "        screen_size=84,\n",
        "        scale_obs=False,\n",
        "    )\n",
        "    base_env = FrameStackObservation(base_env, stack_size=4)\n",
        "\n",
        "    env = RecordVideo(\n",
        "        base_env,\n",
        "        video_folder=video_folder,\n",
        "        episode_trigger=lambda ep_id: ep_id == 0,  # record first episode\n",
        "        name_prefix=\"spaceinvaders_ppo_greedy\",\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    action_dim = env.action_space.n\n",
        "    model = PPOAtariPolicy(action_dim).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    model.eval()\n",
        "\n",
        "    #Run one greedy episode\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    while not done:\n",
        "        obs_t = preprocess_obs(obs[None, ...]) \n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(obs_t)\n",
        "            action = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "\n",
        "    env.close()\n",
        "    print(f\"Recorded greedy episode: reward={total_reward}, steps={steps}\")\n",
        "    print(f\"Video saved under folder: {video_folder}\")\n",
        "\n",
        "# Run\n",
        "record_greedy_spaceinvaders_video()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
